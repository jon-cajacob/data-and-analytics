"use strict";(self.webpackChunkdata_and_analytics=self.webpackChunkdata_and_analytics||[]).push([[8028],{3905:(e,a,t)=>{t.d(a,{Zo:()=>f,kt:()=>m});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),c=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},f=function(e){var a=c(e.components);return r.createElement(l.Provider,{value:a},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},d=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,f=s(e,["components","mdxType","originalType","parentName"]),p=c(t),d=n,m=p["".concat(l,".").concat(d)]||p[d]||u[d]||i;return t?r.createElement(m,o(o({ref:a},f),{},{components:t})):r.createElement(m,o({ref:a},f))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,o=new Array(i);o[0]=d;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[p]="string"==typeof e?e:n,o[1]=s;for(var c=2;c<i;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},8397:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=t(7462),n=(t(7294),t(3905));const i={sidebar_position:121,title:"Using PySpark in a Fabric Lakehouse",slug:"/wiki/msfabric/pyspark",tags:["Data"]},o="Using PySpark in a Fabric Lakehouse",s={unversionedId:"wiki/msfabric/Using PySpark in a Fabric Lakehouse",id:"wiki/msfabric/Using PySpark in a Fabric Lakehouse",title:"Using PySpark in a Fabric Lakehouse",description:"Loading data to a dataframe from the files root",source:"@site/docs/03-wiki/03-msfabric/121-Using PySpark in a Fabric Lakehouse.md",sourceDirName:"03-wiki/03-msfabric",slug:"/wiki/msfabric/pyspark",permalink:"/wiki/msfabric/pyspark",draft:!1,tags:[{label:"Data",permalink:"/tags/data"}],version:"current",sidebarPosition:121,frontMatter:{sidebar_position:121,title:"Using PySpark in a Fabric Lakehouse",slug:"/wiki/msfabric/pyspark",tags:["Data"]},sidebar:"tutorialSidebar",previous:{title:"Loading a table from a Semantic Model to a Lakehouse",permalink:"/wiki/msfabric/dataload4"},next:{title:"SQL",permalink:"/category/sql"}},l={},c=[{value:"Loading data to a dataframe from the files root",id:"loading-data-to-a-dataframe-from-the-files-root",level:4}],f={toc:c};function p(e){let{components:a,...t}=e;return(0,n.kt)("wrapper",(0,r.Z)({},f,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"using-pyspark-in-a-fabric-lakehouse"},"Using PySpark in a Fabric Lakehouse"),(0,n.kt)("h4",{id:"loading-data-to-a-dataframe-from-the-files-root"},"Loading data to a dataframe from the files root"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-jsx",metastring:'title="Reading data from files to a dataframe"',title:'"Reading',data:!0,from:!0,files:!0,to:!0,a:!0,'dataframe"':!0},'from pyspark.sql.functions import col, year, month, quarter\n\ntable_name = \'fact_sale\'\n\ndf = spark.read.format("parquet").load(\'Files/wwi-raw-data1/full/fact_sale_1y_full\')\ndf = df.withColumn(\'Year\', year(col("InvoiceDateKey")))\ndf = df.withColumn(\'Quarter\', quarter(col("InvoiceDateKey")))\ndf = df.withColumn(\'Month\', month(col("InvoiceDateKey")))\n\ndf.write.mode("overwrite").format("delta").partitionBy("Year","Quarter").save("Tables/" + table_name)\n')))}p.isMDXComponent=!0}}]);