"use strict";(self.webpackChunkdata_and_analytics=self.webpackChunkdata_and_analytics||[]).push([[9877],{2616:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"04-01-applying-the-dry-principle-to-effectively-manage-and-organize-measures-in-power-bi","metadata":{"permalink":"/data-and-analytics/blog/04-01-applying-the-dry-principle-to-effectively-manage-and-organize-measures-in-power-bi","source":"@site/blog/2022-11-28-modelling-dry-principle.md","title":"04.01 | Applying the DRY-Principle to Effectively Manage and Organize Measures in Power BI","description":"/// Key performance indicators (KPIs) are at the very core of each analytics solution. It is very common to have a high number of variations of KPIs required to be implemented in such a tool, e.g. variations based on time (current year, previous year, YTD etc.). Further, KPIs are often interlinked and build on each other in KPI trees. The DRY-principle (don\u2019t-repeat-yourself) is a method to make sure this big number of defined KPIs in a solution is properly managed and organized. Specifically, calculation logics are only ever defined in one place. Variations of KPIs and dependable KPIs always reference back to this original definition and nothing is ever repeated.","date":"2022-11-28T00:00:00.000Z","formattedDate":"November 28, 2022","tags":[{"label":"Data Model","permalink":"/data-and-analytics/blog/tags/data-model"},{"label":"DAX","permalink":"/data-and-analytics/blog/tags/dax"},{"label":"Measures","permalink":"/data-and-analytics/blog/tags/measures"}],"readingTime":5.39,"hasTruncateMarker":true,"authors":[{"name":"Jon Cajacob, CFA, FRM","title":"Senior Analytics Projektleiter bei DataVision AG","url":"https://www.linkedin.com/in/jon-cajacob-cfa-frm-4876857b/","imageURL":"https://github.com/jon-cajacob.png","key":"jon"}],"frontMatter":{"slug":"04-01-applying-the-dry-principle-to-effectively-manage-and-organize-measures-in-power-bi","title":"04.01 | Applying the DRY-Principle to Effectively Manage and Organize Measures in Power BI","authors":"jon","tags":["Data Model","DAX","Measures"]},"nextItem":{"title":"05.01 | Your BI-Tool Cannot Fix Data Quality Issues. But It Can and Will Make them Transparent.","permalink":"/data-and-analytics/blog/05-01-make-data-quality-transparent-with-bi"}},"content":"/// Key performance indicators (KPIs) are at the very core of each analytics solution. It is very common to have a high number of variations of KPIs required to be implemented in such a tool, e.g. variations based on time (current year, previous year, YTD etc.). Further, KPIs are often interlinked and build on each other in KPI trees. The DRY-principle (don\u2019t-repeat-yourself) is a method to make sure this big number of defined KPIs in a solution is properly managed and organized. Specifically, calculation logics are only ever defined in one place. Variations of KPIs and dependable KPIs always reference back to this original definition and nothing is ever repeated.\\n\\n\x3c!--truncate--\x3e\\n![Bild](/img/img_04.01-1.png)\\n\\n\\n---\\n\\n\\n\x3c!-- <mark style={{backgroundColor: \'orange\'}}>Text</mark> --\x3e\\n\\n### Introduction\\nIn every BI & analytics project, KPIs are a crucial element of the solution. They have to be defined and then implemented in the tool accordingly, applying specific logic and most of the time mathematical calculations.\\n\\nIt is not uncommon for the sheer number of different KPIs as part of an analytics solution to be quite high. This is mostly driven by numerous **variations** a certain KPI has in the BI tool based on customer requirements. Taking the example of [Sales], variations can be [Sales Previous Year], [Sales YTD], [Sales Previous Year YTD], [Sales Forecast] etc. Further, KPIs often build on each other in so called **KPI trees** (visualized in the picture above).\\n\\nGiven a high number of KPIs which are also often interlinked due to their calculation logic, it is important to organize and manage them in a good way. In this article, I want to introduce a very effective and important method for doing that: The DRY-principle.\\n\\n### What is the DRY-Principle?\\nDRY stands for **Don\u2019t-Repeat-Yourself** and is a pattern originating from the software development world. Explained in plain language, it means that each element of a (software) solution should only be defined once in order to reduce repetitions and redundancy.\\n\\nThis principle is important and effective because it means that when a certain definition or pattern has to be changed, it has to be done in only one place and all depending objects (should) change predictably.\\n\\nLet\u2019s apply this principle to the definition and management of KPIs or custom calculations in a BI & analytics solution. In the following, I will demonstrate this with **Microsoft Power BI** with which KPIs are defined via so called **Measures**.\\n\\nMeasures in Power BI are created applying **DAX**, which is a very powerful functional programming language to implement even the most complicated calculations if required. Measures are **dynamic** and calculated \u201cat runtime\u201d while the user interacts with a report, e.g. changes a filter or drills down a hierarchy.\\n\\n### Applying the DRY-Principle for Measures in Power BI\\nI will demonstrate the application of the DRY-principle using again the example for [Sales] in a hypothetical analytics solution for sales performance measurement. Please note, the DAX language is used here but it is not essential for you to understand it fully to follow the examples.\\n\\nFirst, I will show a very common set of Measures which are all based on the same base KPI [Sales] but vary on the date dimension:\\n\\n| Measure | Definition | Remarks |\\n|---|---|---|\\n| [Sales]| <code>SUM( factSales[Value in CHF] )</code> | This is the base measure which simply takes the sum of the column containing the relevant values for sales volume |\\n| [Sales PY] | <code>CALCULATE( [Sales], DATEADD(dimDate[Date], -12, MONTH))</code> | In order to make direct year-over-year comparisons, a previous year (PY) variation of [Sales] is created which simply references [Sales] and goes back 12 months on the date dimension |\\n| [\u0394 Sales CY vs PY] | <code>[Sales] \u2013 [Sales PY]</code> | The absolute difference between current and previous year sales |\\n| [\u0394% Sales CY vs PY] | <code>DIVIDE( [Sales] \u2013 [Sales PY], [Sales PY] )</code> | The percentage difference between current and previous year sales |\\n| [Sales YTD] | <code>TOTALYTD( [Sales], dimDate[Date] )</code> | Year-to-date sales |\\n| [Sales PY YTD] | <code>TOTALYTD( [Sales PY], dimDate[Date] )</code> | Year-to-date sales of the previous year |\\n| (\u2026) |  | etc. |\\n\\nAs you can see, each measure builds on a previously defined measure. If for example the original table column containing the [Values in CHF] is changed in the data transformation process, this has to be changed in only one measure. All other measures are immediately changed as well in a predictable way.\\n\\nTaking this further, there are often requirements for special variations of a certain base KPI based on a set of **filters**. Also here we also apply the DRY-principle with no problem:\\n\\n| Measure | Definition | Remarks |\\n|---|---|---|\\n| [Sales DACH] | <code>CALCULATE( [Sales], dimRegion[Region] = \u201cDACH\u201d )</code> | We reference again the base measure [Sales] but filter it within the measure for only the region \u201cDACH\u201d |\\n| [Sales PY DACH] | <code>CALCULATE( [Sales PY], dimRegion[Region] = \u201cDACH\u201d )</code> | Same as above, but with reference to [Sales PY] |\\n| (\u2026) |  | etc. |\\n\\nGoing even further, it is often required to reference multiple base measures to build a KPI:\\n\\n| Measure | Definition | Remarks |\\n|---|---|---|\\n| [Material Costs] | <code>SUM( factMatCosts[Value in CHF] )</code> | Base measure for the material costs |\\n| [Contribution Margin I] | <code>[Sales] \u2013 [Material Costs]</code> | The KPI contribution margin in this example is defined as the difference between sales and material costs |\\n| [% Contribution Margin I] | <code>DIVIDE( [Contribution Margin I], [Sales] )</code> | Simple margin calculation which references other measures |\\n| (\u2026) |  | etc. |\\n| [Contribution Margin I PY] | <code>[Sales PY] \u2013 [Material Costs PY]</code> | Previous year variation which references two PY base measures |\\n| (\u2026) |  | etc. |\\n\\nHere again, if the definition of e.g. the Contribution Margin I changes, it has to be adjusted in only one place.\\n\\n### Conclusions\\nApplied rigorously, the DRY-principle allows changes to KPIs in a BI solution to be implemented fast and with high certainty for proper functionality (after the change). It is thus an important factor for long-term operations and further development of a BI tool.\\n\\nThe examples presented here are based on Microsoft Power BI but can certainly applied to any other BI tool (if it allows for cross-referencing in custom calculations).\\n\\nDRY is simple and highly effective and it should be applied by anyone striving for a clean solution design and best practice."},{"id":"05-01-make-data-quality-transparent-with-bi","metadata":{"permalink":"/data-and-analytics/blog/05-01-make-data-quality-transparent-with-bi","source":"@site/blog/2022-11-05-reporting-dataquality-1.md","title":"05.01 | Your BI-Tool Cannot Fix Data Quality Issues. But It Can and Will Make them Transparent.","description":"/// Managing and optimizing data quality is a process of continuous improvement and in many cases strongly driven by an organization\'s culture. Adequate quality of relevant data is crucial for data-driven decision making processes. A business intelligence (BI) solution can make quality issues transparent and help to monitor and track the improvement over time. A BI-tool should however not be used to fix problems for various reasons. This article gives an overview and advice on dealing with data quality.","date":"2022-11-05T00:00:00.000Z","formattedDate":"November 5, 2022","tags":[{"label":"Data Visualization & Reporting","permalink":"/data-and-analytics/blog/tags/data-visualization-reporting"},{"label":"Data Quality","permalink":"/data-and-analytics/blog/tags/data-quality"}],"readingTime":7.59,"hasTruncateMarker":true,"authors":[{"name":"Jon Cajacob, CFA, FRM","title":"Senior Analytics Projektleiter bei DataVision AG","url":"https://www.linkedin.com/in/jon-cajacob-cfa-frm-4876857b/","imageURL":"https://github.com/jon-cajacob.png","key":"jon"}],"frontMatter":{"slug":"05-01-make-data-quality-transparent-with-bi","title":"05.01 | Your BI-Tool Cannot Fix Data Quality Issues. But It Can and Will Make them Transparent.","authors":"jon","tags":["Data Visualization & Reporting","Data Quality"]},"prevItem":{"title":"04.01 | Applying the DRY-Principle to Effectively Manage and Organize Measures in Power BI","permalink":"/data-and-analytics/blog/04-01-applying-the-dry-principle-to-effectively-manage-and-organize-measures-in-power-bi"},"nextItem":{"title":"01.03 | Getting Started - How to Plan and Execute the First Sprint in Your BI & Analytics Project","permalink":"/data-and-analytics/blog/01-03-first-sprint-bi-project"}},"content":"/// Managing and optimizing data quality is a process of continuous improvement and in many cases strongly driven by an organization\'s culture. Adequate quality of relevant data is crucial for data-driven decision making processes. A business intelligence (BI) solution can make quality issues transparent and help to monitor and track the improvement over time. A BI-tool should however not be used to fix problems for various reasons. This article gives an overview and advice on dealing with data quality.\\n\\n\x3c!--truncate--\x3e\\n![Bild](/img/img_05.01-1.png)\\n\x3c!--<div align=\\"center\\"><font size= \\"2\\">Right-click and </font></div>--\x3e\\n\\n---\\n\\n### Introduction\\nIn every data & analytics project I was part of so far in my professional life, data quality was an issue. It is an **evergreen** in the data management space that will likely never go away. As long as humans interact with IT systems, there will be inaccuracies, inconsistencies, incompleteness and many more issues with data in such systems.\\n\\nThere are many ways and tools to identify, categorize and fix quality issues out there. With this short article, I want to share what in my experience works, in particular how to leverage the possibilities a BI & analytics tool gives us with dealing with that problem.\\n\\n### Please Don\u2019t Try to Fix Data Quality Issues with Your BI-Tool\\nWhen **implementing a BI-tool**, data quality problems become apparent quickly. Whether it be incomplete fields, duplicate entries or lacking referential integrity, these issues are very visible once data is transformed and aggregated in reports and dashboards for analysis.\\n\\nModern BI & analytics tools like **Power BI from Microsoft** are incredibly flexible and powerful in dealing with low quality data. It often requires only a few clicks to \u201cquick-fix\u201d issues. Let\u2019s just find-and-replace this wrong value. Or how about we use a manual input mapping table to replace an incomplete categorization of products?\\n\\nBecause so many \u201cquick-fixes\u201d are at hand, it is very tempting to actually implement them directly in the BI-tool. There are however several **problems** attached to such an approach:\\n\\nQuality issues are only fixed at the moment of fixing them. Organizations change over time and so does their data\\nWhen fixing problems in the BI-tool, they are still wrong at the source and other applications have to deal with them again\\nWith each fix being implemented within the BI-tool (and with that I also mean as part of the data cleansing and transformation process), the solution becomes more burdened with an increasing number of additional necessary steps, exceptions and special rules in the data process. This increases the complexity of the solution and the data process and hence decreases operational reliability going forward\\nGiven that, it is obvious that one should always strive to **improve the data quality directly at the source** and not as part of the data value chain from data source to visualization.\\n\\nThe BI solution however can be the perfect tool to **visualize and monitor** the status and improvements of data quality. More on that further below.\\n\\n:::tip\\nAlways strive to fix data quality problems as close to the (data) source as possible.\\n:::\\n\\n### Types of Data Quality Issues\\nPlease note, categorizing data quality issues into types is not very important by itself. But it can help to better identify problems and structure the strategy to fix them.\\n\\nThere are different ways to categorize data quality issues. The following is my view and certainly not the only truth (or even complete).\\n\\n| Data Quality Type | Description / Example |\\n|---|---|\\n| Duplicate data | Usually this is the case for duplicate entries in master data tables. For example, a customer is recorded twice with a slightly different name |\\n| Incomplete data | Some fields are always or mostly empty. Or, when some data is just simply not collected at all. For example, customers which are not categorized into an industry bucket (which would be important for analytics) |\\n| Inconsistent data (across systems) | Usually a dominant problem in organizations with many systems in place. For example, when products are simultaneously recorded in a sales system as well as an ERP-system used for production |\\n| Lack of referential integrity and erroneous relationship cardinality | When transactional data is recorded (e.g. a sales transaction), this transactional data should have a defined relationship with dimensional data (master data). For example, a sales transaction usually refers to one or many products and one customer via unique keys |\\n| Lack of standardization of master data | For example, the naming of countries is not standardized across systems (e.g. \u201cUnited States\u201d, \u201cUnited States of America\u201d, \u201cUSA\u201d, \u201cUS\u201d etc.) |\\n| Inaccurate and erroneous data | E.g. simple misspellings on data entry |\\n| Lack of currency | When data becomes outdated over time (e.g. change of customer address) |\\n| Lack of data validation | When there is no validation if a data entry conforms to the required format. A classical example is the e-mail address format |\\n| and more \u2026 |  |\\n\\n### Ways to Improve Data Quality Over Time\\nIf someone would ask me: \u201cHow would you fix our data quality issues?\u201d, this would be my answer:\\n1. Make data quality issues **transparent** and clearly **identify** which ones are **important and need to be improved and monitored** going forward. This means there are some issues, which are probably **less important** and can be fixed later or even be ignored. Don\u2019t try to fix everything at the same time\\n2. Make sure you understand the **causes** of the identified issues and based on that define and operationalize a strategy to improve the identified issues going forward. Clearly define **responsibilities** in your organization to \u201cown\u201d certain data domains and their quality metrics (e.g. \u201cHR data\u201d). Please note, strategies to fix issues are usually specific to the data domain and systems involved\\n3. Consider thinking about data quality as a **cultural topic** in your organization. In the end, it is important that all people interacting with data in the systems are aware and know the importance of good quality data. **Educate and sensitize** your people accordingly\\n4. Consider using a **data cataloging** system in order to have a dictionary of the data available in your organization, describing its content, quality, lineage and more\\n5. **Continuously monitor** data quality, ideally with a BI-tool. Define and visualize **metrics** to track the improvement of quality over time\\n\\nFurther, there are dedicated **software tools** available on the market to deal with data quality problems and the use of AI (machine learning) in that scope is on the rise. Maybe such dedicated tools are helpful in some way. However, I believe once the issues are **transparent**, a **strategy** with clear **responsibilities** is implemented and **people** are aware of how important good quality data is, a continuous improvement will be visible soon even without an additional software.\\n\\n### Monitoring Data Quality with a BI-Tool\\nThe primary purpose of a BI & analytics solution is to extract, prepare and visualize data relevant for decision making processes. Once source systems are connected, data models are built and KPIs are visualized, data quality issues become apparent quickly. At this point, the BI solution is actually the perfect tool at hand to visualize, monitor and track data quality.\\n\\nHere are some **examples** of how to visualize data quality issues with a BI-tool:\\n\\n- Listing all products appearing in transactional data for which there is no matching master data (i.e. lack of referential integrity)\\n- Measuring the incompleteness of a field (i.e. the percentage of nulls of a field)\\n- Listing and measuring duplicate master data entries (different ways to do that)\\n- Listing of unique attributes of a master data table and detection of depreciated or redundant entries. For example, the segmentation of customers into industries based on which sales performance is measured over time \u2013 some segments are maybe not relevant anymore or some are redundant with slightly different labels across systems\\n\\nAs mentioned, always focus on the **relevant** quality issues and visualize those (not all quality issues are relevant to data-driven processes). And make sure the people being responsible for data and its quality have access to this monitoring dashboard(s).\\n\\n### Conclusions\\nAs long as there is data there will be quality issues with it. We saw there are different **types** of such issues, which can help to better structure the detection and improvement of problems. Improving quality of data is a **continuous improvement process**. Even more, it is a **cultural topic** within the organization. It is crucial to educate and sensitize **people** of its importance.\\n\\nTalking about hands-on advice, I can recommend to use your BI-tool to **visualize** and with that make data quality issues transparent. Don\u2019t try to fix these issues within the tool though, it will result in bad solution design and low operational reliability in the long term.\\n\\nIf issues are transparent and being monitored, the success of quality improvement strategies can be measured. **Strategies** to improve data quality are specific to the data domain and systems. Make sure there are clear **responsibilities** for data in your organization. Given that, improvements in data quality should be visible soon."},{"id":"01-03-first-sprint-bi-project","metadata":{"permalink":"/data-and-analytics/blog/01-03-first-sprint-bi-project","source":"@site/blog/2022-10-04-project-mgmt-first-sprint.md","title":"01.03 | Getting Started - How to Plan and Execute the First Sprint in Your BI & Analytics Project","description":"/// The first sprint in a BI & analytics project is foundational for both the analytics solution as well as how the project is organized going forward. Structuring and managing a BI project in sprints is not mandatory, but can help to organize work in an effective way. In this article I summarize based on my experience how a first sprint could look like in order to have the best possible start in your BI & analytics journey.","date":"2022-10-04T00:00:00.000Z","formattedDate":"October 4, 2022","tags":[{"label":"Project Management","permalink":"/data-and-analytics/blog/tags/project-management"},{"label":"Agile","permalink":"/data-and-analytics/blog/tags/agile"}],"readingTime":7.22,"hasTruncateMarker":true,"authors":[{"name":"Jon Cajacob, CFA, FRM","title":"Senior Analytics Projektleiter bei DataVision AG","url":"https://www.linkedin.com/in/jon-cajacob-cfa-frm-4876857b/","imageURL":"https://github.com/jon-cajacob.png","key":"jon"}],"frontMatter":{"slug":"01-03-first-sprint-bi-project","title":"01.03 | Getting Started - How to Plan and Execute the First Sprint in Your BI & Analytics Project","authors":"jon","tags":["Project Management","Agile"]},"prevItem":{"title":"05.01 | Your BI-Tool Cannot Fix Data Quality Issues. But It Can and Will Make them Transparent.","permalink":"/data-and-analytics/blog/05-01-make-data-quality-transparent-with-bi"},"nextItem":{"title":"02.01 | How to Choose the Right BI & Analytics Tool for Your Organization (5 Criteria)","permalink":"/data-and-analytics/blog/02-01-how-to-choose-bi-and-analytics-tool"}},"content":"/// The first sprint in a BI & analytics project is foundational for both the analytics solution as well as how the project is organized going forward. Structuring and managing a BI project in sprints is not mandatory, but can help to organize work in an effective way. In this article I summarize based on my experience how a first sprint could look like in order to have the best possible start in your BI & analytics journey.\\n\\n\x3c!--truncate--\x3e\\n![Bild](/img/img_01.03-1.png)\\n\\n\\n---\\n\\n\\n\x3c!-- <mark style={{backgroundColor: \'orange\'}}>Text</mark> --\x3e\\n\\n### Introduction\\nIt is decided: You want to embark on a journey to develop, implement and apply a business intelligence solution in your organization. But where and how to start exactly? In the beginning of any BI-project it is important to have a clearly defined **scope and focus** for the first project deliverables. Further, the first sprint will likely lay some of the foundation of many elements of the full solution (e.g. ETL process, data model definition etc.). Finally, in some organizations it can be critical to \u201cproof\u201d the value of the project with the first sprint, e.g. with a proof-of-concept (PoC).\\n\\nTherefore, the first sprint is important. In this article, I want to give you the tools and ideas to have the best possible start in your project.\\n\\n### What is a Sprint?\\nIn agile **project management**, a project is broken down into sprints on the timeline. One sprint can last 2, 3 or 4 weeks etc.. Each sprint is **planned** and then executed in order to achieve the **sprint goal** which is to deliver a working element of a bigger solution. After the sprint, the progress and outcome is **reviewed** together with the customer.\\n\\nThe sprint goal is important and tells us which tasks have to be prioritized and executed in the sprint. Generally, it is recommended to derive the sprint goal from the **user\u2019s view**, using the following template:\\n\\n> As a **[user]**, I want to **[goal]**, so that **[reason]**.\\n\\n*Example:*\\n\\n> *As a Controller, I want to analyze Total Third Party Revenue by Product Category, Customer Branche and Region, and Date, so that I can understand and identify trends, important segments and patterns and report findings to the CEO and CFO.*\\n\\nAs you can see, we are basically formulating an **analytical question** from the user\u2019s point of view. Based on this analytical question, we can immediately derive the required **KPIs** and **dimensionality** of the BI solution:\\n\\nKPI: Total Third Party Revenue\\nDimensions: Product, Customer, Date\\nThe **reasoning** in the goal statement, tells us how the data should be presented (visualized). I recommend doing this as one of the last steps in the sprint though.\\n\\nPlease note: There are quite a bit more details which could be told about agile project management and sprints. However, it is not the goal of this article to do such a deep dive. If you want to learn more details, I can recommend the wiki from [<ins>Atlassian</ins>](https://www.atlassian.com/agile/scrum/sprints).\\n\\n### Sprint Planning for BI-Projects - Step by Step\\n*One note before we start:* Don\u2019t try to plan and anticipate everything and every detail with planning. It will not be possible as many things are yet unknown. You will embark on a learning path with the project. Take these learnings and incorporate them in the process along the way. After all, this is the agile way of doing a project.\\n\\nHere is my recommendation for effective sprint planning for a BI-Project:\\n\\n1. Define the **analytical question(s)** (sprint goal), which need to be answered with the BI solution\\n2. Derive **KPI definitions** and the required dimensionality from the analytical question(s)\\n    - I recommend using a simple template: See my separate article about an effective KPI template\\n3. Derive and draft the **data model(s)** from the KPIs and their dimensionality\\n4. Identify required data sources and necessary data transformation process(es) (ETL)\\n5. Build the **backlog of tasks** which need to be done in order to reach the sprint goal\\n6. *If required:* Discuss and define **details about how** the solution shall be developed (e.g. define if there will be a direct connection to a source system or if the BI will access only data exports from that system)\\n\\nNext to the analytical questions, you will need to define and implement the **technical framework** for the solution in the first sprint. This includes usually the following elements:\\n\\n- Setting up **user credentials** to access source systems\\n- **Installing** and **licensing** of the **BI & analytics software** (e.g. Power BI)\\n- Setting up the **cloud environment** in which the solution shall be hosted, e.g. the Power BI Tenant\\n- Installing and setting up a **data gateway** (e.g. Power BI Gateway) in order to allow data transports between on-premise systems and the cloud environment for the BI tool\\n\\nAlso, I recommend identifying and actively managing **dependencies**. Often times you have critical dependencies regarding organizational sign-offs on e.g. user credentials or data protection. Such dependencies can put your project quickly on-hold.\\n\\nFinally, you obviously have to define the **sprint length in weeks**. Because the first sprint is foundational, I would recommend at least four weeks of time for it. After that, you could reduce to two or three weeks per sprint.\\n\\n### Sprint Execution\\nExecuting the sprint means working through the backlog of tasks to reach the sprint goal. I would recommend to track the progress with a simple **Kanban board**:\\n\\n![Simple Kanban Board](/img/img_01.03-2.png)\\n<div align=\\"center\\"><font size= \\"2\\">A simple but effective Kanban board to manage and visualize the progress of work</font></div>\\n<br/>\\n\\nTasks move from the backlog to \u201cin progress\u201d to finally \u201cdone\u201d. The board helps to keep track of and visual the progress of the sprint execution. There are plenty of tools available to use such a board. If your organization uses Microsoft Teams, check out the Planner application.\\n\\nPlease note, in case you do not want to use the sprint method in your project (and that would be totally acceptable) you could still use the Kanban board (independent from sprints) as the key tool to manage the project progress.\\n\\nHere is a more sophisticated board with more statuses and grouping of tasks:\\n\\n![More Elaborate Kanban Board](/img/img_01.03-3.png)\\n<div align=\\"center\\"><font size= \\"2\\">A more elaborate Kanban board with grouping of tasks (swimlines) and an additional status column</font></div>\\n<br/>\\n\\n### Sprint Review\\nAt the end of a sprint, the progress of work and the results / deliverables are reviewed together with the customers. Based on the deliverables, it is measured if the **sprint goals** have been achieved. This usually requires the **customer to test** the new elements of the solution (and the current state of the solution overall) and provide **feedback** accordingly, which is then incorporated in the backlog for the next sprint to follow the principle of **continuous improvement**.\\n\\nPlease note, this means the customer has to have capacity to review things and respond within due time accordingly. You may want to consider in your planning, that this takes more time than initially anticipated.\\n\\nFurther, if deemed useful, a sprint **retrospective** can be organized in which the team reflects on how the work was organized and done, and improve accordingly.\\n\\n### Warning: Don\u2019t Get Lost in Agile Project Management Methodology\\n:::caution\\nWhen planning, executing and reviewing a sprint, make sure the focus is on the **delivery of working solutions** at all times and don\u2019t get lost in planning and managing the project.\\n:::\\n\\nA BI & analytics projects has a lot of uncertainties involved and it will not be possible to remove all of them with planning or agile methods. Instead, consider the project as a learning journey for your people and the organization. As already noted, embrace the uncertainty and take these learnings and incorporate them into the process right away.\\n\\nThere is a ocean of books, software and online content available on agile project management. There are manifestos, sophisticated tools and a lexicon of fancy words used. There are people that have \u201cagile coaching\u201d as their full time occupation. It is understandable that agile project management can become a jungle where one gets lost and looses the focus in the project which should be the delivery of usable pieces of an analytics solution.\\n\\nTherefore, choose and use the methods which really help you and the team. Keep it simple and focus on the project outcome.\\n\\n### Conclusions\\nStarting a new BI & analytics project is exciting, yet can also be daunting in particular if an organization lacks prior experience with such endeavors. However, if you actively manage the scope and progress of work with simple and effective tools, and focus on the delivery of usable solutions, you are on a good path to success with your project."},{"id":"02-01-how-to-choose-bi-and-analytics-tool","metadata":{"permalink":"/data-and-analytics/blog/02-01-how-to-choose-bi-and-analytics-tool","source":"@site/blog/2022-09-27-solution-design-choosing-bi-tool.md","title":"02.01 | How to Choose the Right BI & Analytics Tool for Your Organization (5 Criteria)","description":"/// Choosing the right BI & Analytics software tool is an important decision for any organization embarking on the journey to become (more) data-driven. This article summarizes the 5 most important criteria to consider in the tool selection process.","date":"2022-09-27T00:00:00.000Z","formattedDate":"September 27, 2022","tags":[{"label":"Solution Design","permalink":"/data-and-analytics/blog/tags/solution-design"},{"label":"BI Software Market","permalink":"/data-and-analytics/blog/tags/bi-software-market"}],"readingTime":5.275,"hasTruncateMarker":true,"authors":[{"name":"Jon Cajacob, CFA, FRM","title":"Senior Analytics Projektleiter bei DataVision AG","url":"https://www.linkedin.com/in/jon-cajacob-cfa-frm-4876857b/","imageURL":"https://github.com/jon-cajacob.png","key":"jon"}],"frontMatter":{"slug":"02-01-how-to-choose-bi-and-analytics-tool","title":"02.01 | How to Choose the Right BI & Analytics Tool for Your Organization (5 Criteria)","authors":"jon","tags":["Solution Design","BI Software Market"]},"prevItem":{"title":"01.03 | Getting Started - How to Plan and Execute the First Sprint in Your BI & Analytics Project","permalink":"/data-and-analytics/blog/01-03-first-sprint-bi-project"},"nextItem":{"title":"01.02 | 5 Important Risks to Manage in Every BI-Project Project","permalink":"/data-and-analytics/blog/01-02-5-important-risks-bi-project"}},"content":"/// Choosing the right BI & Analytics software tool is an important decision for any organization embarking on the journey to become (more) data-driven. This article summarizes the 5 most important criteria to consider in the tool selection process.\\n\\n\x3c!--truncate--\x3e\\n![Bild](/img/img_02.01-1.png)\\n\\n\\n---\\n\\n\x3c!-- <mark style={{backgroundColor: \'orange\'}}>Text</mark> --\x3e\\n\\n### Introduction\\nThere are 20 different software tools in the Gartner Magic Quadrant for Business Intelligence Platforms (as of March 2022). Choosing the right tool for your organization can quickly become overwhelming especially when most of them seem to promise \u201ceverything you need\u201d in their individual marketing materials.\\n\\n![Bild](/img/img_02.01-2.jpg)\\n<div align=\\"center\\"><font size= \\"2\\">The Gartner Magic Quadrant for Analytics and Business Intelligence Platforms as of March 2022\\n</font></div>\\n\\nI have seen and used various different market leading tools in different corporate setups. Based on that experience I believe the following should be the leading criteria in selecting the right BI & analytics tool.\\n\\n### Comprehensive coverage of the data & analytics value chain\\n\\nThe following are the generalized steps in the data & analytics value chain:\\n\\n- Data **connection** and **extraction** (from data source)\\n- Data **transformation** (cleansing, preparation etc.)\\n- Data **modeling** (facts and dimensions)\\n- Data **presentation** (visualization)\\n- Publishing and **sharing** of analytics content (e.g. dashboard) with other people within the organization\\n- The more comprehensive a BI & analytics tool can cover these steps, the better.\\n\\nIf for example the data extraction and transformation is <ins>not covered</ins>, you will need to acquire, license and implement yet another tool to do just that. Further, if several tools are needed to cover the value chain, data needs to be transported from one tool to another which requires additional time, computing power and makes the entire solution less robust because of the additional interfaces.\\n\\nPlease note, if your data architecture uses a data warehouse (DWH), data lake or data lakehouse the need for coverage of the analytics value chain is of course different when for example all data transformation is done before data is loaded to the DWH and the BI tool is solely used for data presentation.\\n\\n### Low-code and intuitive to use\\n\\nLow-code means that the necessity to use programming / coding in the tool is limited because most functionality is available via **buttons, dropdowns and drag-and-drop** in the tool interface. This is a very important requirement if you want to enable your business users to directly work with the tool and become more **data literate**.\\n\\nNote: The more code that is required to work with a tool, the harder **adoption** by business users will become in your organization. Because by nature, business users are not programmers. However, many are used to work with Excel formulas and hence have at least some basic idea of working with logical function statements.\\n\\nClosely related to this criteria is the functionality to use programming driven by more complex analytics requirements. Many tools for example allow the direct use of **Python or R** code to build more advanced analytics content and even machine learning applications.\\n\\n### Large user community\\n\\nA large, worldwide user community for a software means it is **easy to find help and resources online** (e.g. via Google search) if one runs into a specific problem (or wants to self-teach the tool). It also means the tool should not be too bad to work with, otherwise there would not be a large user community.\\n\\nThe user community is not to underestimate because the easier one finds solutions online the more efficient it becomes to build analytics with the tool (and the less frustrating it is). Further, your organization will be less dependent on external experts because internal business users can solve problems themselves.\\n\\nAlso note, it is easier to find and recruit people which have the skills in working with a broadly known and applied software (or programming language, like for example SQL).\\n\\n**How can I estimate the popularity and size of a user community?**\\n\\nYou could use the [<ins>Google Trend Analysis</ins>](https://trends.google.com/trends/?geo=CH) tool to estimate the relative popularity of a given set of tools.\\n\\nTo illustrate this, I ran the tool for the three market leading tools Power BI, Tableau and Qlik Sense for a time range from 1 January 2016 to roughly end of September 2022. There seems to be a strong upward trend for Power BI worldwide while Tableau and Qlik stagnate over time.\\n\\n![Bild](/img/img_02.01-3.png)\\n<div align=\\"center\\"><font size= \\"2\\">Google trend analysis for the three major BI platforms\\n</font></div>\\n<br/>\\n\\n*Please note:* It is a bit tricky to find the right search term as Google categorizes them into groups like \u201cSoftware\u201d or \u201cTopic\u201d. It probably makes sense to play around a bit and get an understanding of the popularity evolution of each tool.\\n\\n### Easy integration and many data connectors\\n\\nWhen you acquire and implement a new software, you want to make sure that it integrates well with your existing IT infrastructure. Most importantly, it should be no problem to connect with your relevant source systems or file repositories (e.g. Sharepoint) and extract data with good performance. Therefore, make sure the selected tool has many data connectors built into it.\\n\\nFurther, be very careful if a \u201ccustom connector\u201d is needed in addition to the BI tool to connect with a source system. Such **custom connectors** are often built by very small teams / companies (or even just a single person) and you don\u2019t want to make your entire BI landscape dependent on those.\\n\\n### Low infrastructure requirements and license costs\\n\\nThe new tool should be easy to implement and maintain, and should not cause an extensive IT overhead. Further, it should be easy to scale in your organization. Modern BI & analytics tools therefore often run in the cloud and can as such be easily maintained and accessed by many users.\\n\\nFinally, obviously keep license costs in mind in your selection process (but don\u2019t give too much weight to this criteria as the return in value of the new tool will quickly \u201cpay back\u201d such costs).\\n\\n### Conclusions\\n\\nWhen successfully implemented and applied, a BI & analytics tool can bring tremendous value to your organization. It will enable your people to **automate tasks and processes** and to focus on actual data analysis. Your people will become more **data literate**, decisions will be more **data-driven** and your organization as a whole will become more **competitive** as a result."},{"id":"01-02-5-important-risks-bi-project","metadata":{"permalink":"/data-and-analytics/blog/01-02-5-important-risks-bi-project","source":"@site/blog/2022-09-18-project-mgmt-5-risks.md","title":"01.02 | 5 Important Risks to Manage in Every BI-Project Project","description":"/// In this article, 5 important risks to manage in a BI-project are summarized. These risks are about project scope & planning, people, organization and finally the evergreen data quality management. They can be successfully handled with diligence and the right tools.","date":"2022-09-18T00:00:00.000Z","formattedDate":"September 18, 2022","tags":[{"label":"Project Management","permalink":"/data-and-analytics/blog/tags/project-management"},{"label":"Risk Management","permalink":"/data-and-analytics/blog/tags/risk-management"}],"readingTime":8.36,"hasTruncateMarker":true,"authors":[{"name":"Jon Cajacob, CFA, FRM","title":"Senior Analytics Projektleiter bei DataVision AG","url":"https://www.linkedin.com/in/jon-cajacob-cfa-frm-4876857b/","imageURL":"https://github.com/jon-cajacob.png","key":"jon"}],"frontMatter":{"slug":"01-02-5-important-risks-bi-project","title":"01.02 | 5 Important Risks to Manage in Every BI-Project Project","authors":"jon","tags":["Project Management","Risk Management"]},"prevItem":{"title":"02.01 | How to Choose the Right BI & Analytics Tool for Your Organization (5 Criteria)","permalink":"/data-and-analytics/blog/02-01-how-to-choose-bi-and-analytics-tool"},"nextItem":{"title":"01.01 | KPI Definitions as a Core Project Management Tool (Template)","permalink":"/data-and-analytics/blog/01-01-kpi-definitions-template"}},"content":"/// In this article, 5 important risks to manage in a BI-project are summarized. These risks are about project scope & planning, people, organization and finally the evergreen data quality management. They can be successfully handled with diligence and the right tools.\\n\\n\x3c!--truncate--\x3e\\n![Bild](/img/img_01.02-1.png)\\n\\n\\n---\\n\\n\\n\x3c!-- <mark style={{backgroundColor: \'orange\'}}>Text</mark> --\x3e\\n\\nThere are possibly endless reasons why a specific BI-project in a specific organisation is at risk of failure. However, I believe a certain pattern can be observed which I summarize in the following.\\n\\n:::tip\\nKnowing the risks is one thing. Actually managing them requires discipline and hard work. Take this list as a constant reminder in your BI journey and don\u2019t fall for the various traps that might lurk around the next corner.\\n:::\\n\\n### 1. Too much complexity and scope in the beginning and at all times\\nIn my experience, this is maybe the most relevant reason, why a BI-project struggles. Starting a new project is exciting and people are highly motivated, in particular in the beginning. Senior management might saw a very fancy dashboard at a conference and now also wants (exactly) that \u2013 ASAP of course. And it is just a few charts and tables, right?\\n\\nAs a result, huge lists with detailed requirements for e.g. KPIs or data tables with +150 fields are created and put into project backlog. Why is that problematic?\\n\\n- The **team will be overwhelmed** by the backlog: Where to even start? What is important and what not?\\n- Also **customer will be overwhelmed** by the complexity (which is often self-inflicted). How to prioritize and schedule / plan a backlog with ~100 tasks?\\n- Work gets more and more **uncoordinated** as there is **no focus** on a particular topic. Many things are started, nothing is actually usable, validated or reviewed (and improved accordingly)\\n- **Only very late in the project, real and ready-to-test results will be available**. The uncertainty of the project outcome is very high accordingly\\n- Because testing and collecting user feedback is postponed, there is a high risk your **BI journey goes into the wrong direction**. Things have to be changed again. More time (and money) is lost\\n- **Frustration** both in the project team and on the customer side increases \u2026 and so on \u2026\\n\\n<ins>What can I do to manage this risk?</ins>\\n\\nI usually like to give hands-on advice, so here it goes:\\n\\n- Start with **one or two KPIs** (e.g. sales volume and sales margin) \u2192 See my article about structuring KPI requirements with a simple template\\n- Start with **3 \u2013 4 dimensions** (e.g. date calendar, product, customer and account)\\n- Start with only **one report page** with 1 \u2013 3 very basic visualizations (ideally tables only) and some filters\\n- **Validate** the data output (e.g. total sales)\\n- Let the **customer test** the single page so they can provide **feedback**\\n*\u2026 plan and execute the next **sprint** and **iteratively** develop and deliver additional elements of the solution*\\n\\n*Example:* Your first report page might look just like the following, and that is totally fine:\\n\\n![Bild](/img/img_01.02-2.png)\\n<div align=\\"center\\"><font size= \\"2\\">A simple first report page as a result of your first project sprint\\n</font></div>\\n\\n\\n### 2. Know how and experience is lacking in the team\\nThis one is not easy to manage. Building up analytics knowledge (often called \u201cdata literacy\u201d) requires significant investments of money and time. You need to have the right people in terms of both **hard and soft skills** to make BI a success in your company. Hard skills are needed to technically build the solution. Soft skills are crucial to manage the process (or project) around development and successfully implement (including marketing) and operationalize the new BI tool.\\n\\nIn addition, experience is needed in order to make sure the solution is robust in daily operations, i.e. follows certain best practices which are often not written down in textbooks.\\n\\nThe good news is that modern analytics tools do not require extensive programming knowledge anymore (so called \u201clow-code\u201d) and are more and more intuitively to work with. This allows any motivated individual to self-teach herself how to work with such a tool and build a BI solution without dependency on experts.\\n\\n<ins>What can I do to manage this risk?</ins>\\n\\n- Identify highly motivated individuals in your organization and give them the capacity and tools to get started with BI and analytics\\n- Choose a modern and market leading BI & analytics tool with a large worldwide community to use in your organization. Please note, this is not primarily a decision made by IT but by the business users in your organization who actually have to work with the tool\\n- Acquire external experts to coach your internal BI people in order to significantly increase the speed of building data literacy within the organization and to make sure you are following best practice with the solution\\n- If you do not have the capacity or knowledge within the organization, acquire external experts to build a BI solution. However, make sure this is done with a low-code self-service tool which can be handled by your internal BI people in order to not be dependent on external experts\\n\\n### 3. There are not enough people doing actual work\\nFor this risk, I like to think about a analogy of building a house. No matter how many people are involved in planning and building a house, in the end there needs to be a team of individuals which actually builds the house.\\n\\nThe very same goes for a BI-project. No matter how many **managers, product owners or agile coaches** you have on the team, someone at some point has to sit down and do the actual work. The actual hard work of connecting with the data source, transforming the data step by step, building KPI calculations, data visualizations and fixing problems.\\n\\nIt is my personal opinion, that the former type of individual usually gets a little too much attention in an organization. It is important to note and to know at all times that how \u201cgood\u201d a BI solution performs (in terms of many aspects) is very much dependent on the person actually building it. **You can have the most beautiful project plan and list of requirements, if it is not executed well, the project will fail with high probability.**\\n\\n<ins>What can I do to manage this risk?</ins>\\n\\n- Be mindful about the ratio of people \u201cmanaging\u201d or \u201ccoaching\u201d versus the people actually developing and operationalizing a solution and structure the team accordingly\\n\\n### 4. The BI-project is considered *purely* an IT project\\nIf a BI-project is organizationally placed and driven purely out of the IT department without at least comprehensive involvement of the business department(s), such project will unlikely be successful.\\n\\nIn the past, business intelligence was a topic owned almost solely by IT simply because building content with tools back in the days required deep technical expert knowledge (e.g. coding with SQL). It was also very common for example to have a data warehouse in place where data was extracted from source systems and transformed to be used in some front end tool. Such data warehouses were administrated solely by IT experts. That means in the past **the majority or even the whole data analytics value chain from data source to data presentation was owned by IT experts.**\\n\\nIn my experience, today\u2019s framework conditions look different. **Modern analytics tools** do almost not require any coding anymore and can be self-taught by any motivated person. This means analytics content can be produced and owned where it is ultimately needed: directly within in the business department. This allows for **more flexibility, less dependency and most importantly fast implementation of requirements, changes and fixes.**\\n\\n<ins>What can I do to manage this risk?</ins>\\n\\n- Clearly define the **scope of responsibilities** between the business and IT department: Who owns which topic in the analytics value chain?\\n- Same as in 2. above, identify motivated individuals and give them the means and capacity to owning analytics topics, e.g. report creation\\n- In a very progressive state of **data literacy** in your organization designated people in the business departments own the full analytics value chain from data extraction to data visualization\\n\\n### 5. Bad data quality\\n\\nThis is an evergreen and **very critical for the success of your BI solution**. Why?\\n\\nImagine a business user opens a newly implemented dashboard to understand and analyze some data relevant for that person. The user sees the relevant data but quickly realizes that it is **erroneous, incomplete and not up-to-date**. How many more times do you think that particular business user will log into this dashboard again? **Zero** times to be exact. And it is at this point where the new BI tool slowly becomes irrelevant in the organization because \u201c.. it anyway shows the wrong data ..\u201c.\\n\\nI believe some of the **main reasons why data quality remains such a significant problem are**:\\n\\n- Data quality is not really a \u201ccool\u201d topic like for example machine learning and thus does not get the attention it needs in an organization\\n- It is often not transparent where the most pressing data quality issues are\\n- There is no strategy in place to manage data quality with clear responsibilities\\n\\n<ins>What can I do to manage this risk?</ins>\\n\\n- Basically work on the reasons I just listed above: 1) give it more attention, 2) make it transparent (via the BI tool) and 3) put a strategy with clear responsibilities in place to manage the relevant data quality\\n\\nI have written a separate article about the topic of data quality. [See here](2022-11-05-reporting-dataquality-1.md)\\n\\n:::tip\\nNot all data has to have the same level of quality. Therefore, first identify the most relevant data and its issues and work on that one first.\\n:::\\n\\n### Conclusions\\nThere are many more potentially important risks to manage in a BI-project but I believe if you are aware of the ones listed above and you actively monitor and manage them, you are on a good path to success."},{"id":"01-01-kpi-definitions-template","metadata":{"permalink":"/data-and-analytics/blog/01-01-kpi-definitions-template","source":"@site/blog/2022-09-16-project-mgmt-kpi-definitions.md","title":"01.01 | KPI Definitions as a Core Project Management Tool (Template)","description":"/// Key performance indicators are the core of each BI solution and should therefore be defined clearly. A simple template to list KPI definitions can be a valuable and central tool in the agile BI project for the entire team including the customer.","date":"2022-09-16T00:00:00.000Z","formattedDate":"September 16, 2022","tags":[{"label":"Project Management","permalink":"/data-and-analytics/blog/tags/project-management"},{"label":"Template","permalink":"/data-and-analytics/blog/tags/template"}],"readingTime":4.195,"hasTruncateMarker":true,"authors":[{"name":"Jon Cajacob, CFA, FRM","title":"Senior Analytics Projektleiter bei DataVision AG","url":"https://www.linkedin.com/in/jon-cajacob-cfa-frm-4876857b/","imageURL":"https://github.com/jon-cajacob.png","key":"jon"}],"frontMatter":{"slug":"01-01-kpi-definitions-template","title":"01.01 | KPI Definitions as a Core Project Management Tool (Template)","authors":"jon","tags":["Project Management","Template"]},"prevItem":{"title":"01.02 | 5 Important Risks to Manage in Every BI-Project Project","permalink":"/data-and-analytics/blog/01-02-5-important-risks-bi-project"}},"content":"/// Key performance indicators are the core of each BI solution and should therefore be defined clearly. A simple template to list KPI definitions can be a valuable and central tool in the agile BI project for the entire team including the customer.\\n\\n\x3c!--truncate--\x3e\\n![Bild](/img/img_01.01-1.png)\\n\\n\\n---\\n\\n\\n\x3c!-- <mark style={{backgroundColor: \'orange\'}}>Text</mark> --\x3e\\n\\nThere are possibly endless reasons why a specific BI-project in a specific organisation is at risk of failure. However, I believe a certain pattern can be observed which I summarize in the following.\\n\\n*One note:* Knowing the risks is one thing. Actually managing them requires discipline and hard work. Take this list as a constant reminder in your BI journey and don\u2019t fall for the various traps that might lurk around the next corner.\\n\\n### Introduction\\nThe purpose of a business intelligence (BI) solution is ultimately to answer business critical questions and with that to support (data driven) decision making. The multidimensional analysis of key performance indicators (KPIs) is at the core of this purpose.\\n\\nIt therefore makes sense to clearly list and define KPIs already in the early beginning of a BI project using a simple template. In the course of the (agile) project, this list of definitions is then continuously updated and extended.\\n\\n\\n### Analytical questions as the starting point\\n\\nWhich KPIs should be part of a BI solution depends on the analytical questions, which have to be answered.\\n\\n*Examples*\\n\\n- What are our top 10 products based on sales volume?\\n- How does the contribution margin of the current period compare to the previous period?\\n- Which sales area had the strongest growth based on order entry in the past 6 months?\\n\\nBased on a catalogue of analytical questions, we can immediately derive necessary KPIs and their dimensionality.\\n\\n### Template: KPI definitions\\n\\nThe list of KPI definitions is a working instrument at the core of the BI project management and is updated in the course of the project as requirements get more clear.\\n\\nI recommend using a simple **Excel spreadsheet** which can also be shared within the project team (e.g. via MS Teams).\\n\\n![Bild](/img/img_01.01-2.png)\\n<div align=\\"center\\"><font size= \\"2\\">Example of the KPI Definitions Template (right-click and open in new tab/window to enlarge)\\n</font></div>\\n\\n### Features of a KPI definition\\n\\n| Feature | Description | Example |\\n|---|---|---|\\n| KPI label | Ideally short, descriptive and structured | Sales Volume |\\n| Category / grouping | If useful: Grouping of KPIs for a better overview | Sales KPI |\\n| Source | Source system, table, field | SQL ABC, tbl_Sales, [Amount] |\\n| Formula | Description of the calculation. Can be done mathematically, in text or even directly with DAX | Sum of [Amount] or SUM([Amount]) |\\n| Pre-Filtering / Data Scope | Many KPIs require pre-filtering of the underlying data table by certain attributes in order to comply with the KPI definition | dimAccount[Account ID] starts with 4* dimCustomer[Intercompany] = False |\\n| Format | The format of the KPI is often times self-explaining. Sometimes though it is necessary to specifically define it | Whole number (0 decimals) and thousand separator |\\n| Dimensions | Based on which dimensions should the KPI be analyzed? (+ are there dimensions, based on which the KPI cannot be analyzed?) | Date (calendar) Customers Products |\\n| Drill hierarchies (aggregations and detail levels) | On which aggregation levels and until which detail level shall the KPI be analyzed? What is the order of aggregations through which the user can drill? | Region \u2192 Customer \u2192 Product category |\\n| Variations | Very often it is necessary to have variations of a certain KPI. For example the previous-year value of the KPI for a delta analysis. It is for a better overview to not list these variations separately in the KPI definitions. | Sales Volume PY \u0394 Sales Volume CY vs PY \u0394% Sales Volume CY vs PY |\\n| Dependencies | KPIs often depend on each other and are sometimes even part of a bigger KPI tree. It is therefore important to note these dependencies (also see: DRY principle) | Used in KPI [Contribution margin] |\\n\\n### Conclusions\\n\\nIn my experience, a detailed project or solution documentation rarely provides the value that one hopes for. Requirements and implemented elements change far too fast to keep track of and with modern BI tools, the **documentation can be done directly inside the tool**. Be it commentary in complex KPI calculations (e.g. with DAX) or descriptions of transformation steps in the ETL (e.g. within Power Query).\\n\\nA list of KPI definitions however, requires little effort but brings extensive value in the course of the project as KPIs are the core of any BI solution. From the KPI definitions we can immediately get a better understanding of how the solution should ultimately look like and easily **derive requirements for the other building blocks of the tool**. Which dimensions are needed as part of the data model? Which facts are needed on which detail level? etc.\\n\\nWhy not give it a try in your project?"}]}')}}]);